{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a61bb43b-24c0-4823-9696-b0acc0af05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2c17ff-f716-497f-8598-ecf412d1c1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: [  0  10  20  30  40  50  60  70  80  90 100]\n",
      "Actions: ['FILL', 'STOP']\n"
     ]
    }
   ],
   "source": [
    "#Define State and Actions \n",
    "states = np.arange(0, 101, 10)   # 0, 10,20,30,40..........100\n",
    "actions = ['FILL', 'STOP']       #Fill or Stop\n",
    "print('States:', states)\n",
    "print('Actions:', actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6dfda1a-7792-4c8c-92ae-d2565f9d6b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table shape: (11, 2)\n"
     ]
    }
   ],
   "source": [
    "#create Q table and set alpha, gamma, epsilon, episodes\n",
    "#Q Table: [s,a]\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "alpha = 0.1     # Learning rate\n",
    "gamma = 0.9     # Discount factor\n",
    "epsilon = 0.2   # Exploration rate\n",
    "episodes = 300  # Training runs\n",
    "\n",
    "print('Q-table shape:', Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c3ff8d2-0cf0-4af0-93ba-5d41c4b99e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Example (level=60, FILL): 10\n"
     ]
    }
   ],
   "source": [
    "#Write Reward , Penality function\n",
    "def get_reward(level, action):\n",
    "    if 40 <= level <= 70:\n",
    "        reward = 10  # ideal range\n",
    "    else:\n",
    "        reward = -10  # too low/high\n",
    "    if action == 'FILL' and level >= 90:\n",
    "        reward -= 10  # overflow risk\n",
    "    if action == 'STOP' and level <= 10:\n",
    "        reward -= 10  # empty risk\n",
    "    return reward\n",
    "\n",
    "print('Reward Example (level=60, FILL):', get_reward(60, 'FILL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d574fa4-b191-4299-9a15-47f3f3a9c65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Level Example: 60\n"
     ]
    }
   ],
   "source": [
    "def next_level(level, action):\n",
    "    if action == 'FILL':\n",
    "        level += random.choice([5, 10, 15])\n",
    "    else:\n",
    "        level -= random.choice([5, 10, 15])\n",
    "    return int(np.clip(level, 0, 100))\n",
    "\n",
    "print('Next Level Example:', next_level(50, 'FILL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73575db8-f80f-4780-b943-1688ffff685e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41e90bc7-3ec3-4ff0-989a-1451b3159622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "for ep in range(episodes):\n",
    "    level = random.choice(states)\n",
    "    for _ in range(15):  # steps per episode\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = actions[np.argmax(Q[level // 10])]\n",
    "\n",
    "        next_state = next_level(level, action)\n",
    "        reward = get_reward(next_state, action)\n",
    "\n",
    "        a = actions.index(action)\n",
    "        best_next = np.max(Q[next_state // 10])\n",
    "        Q[level // 10, a] += alpha * (reward + gamma * best_next - Q[level // 10, a])\n",
    "print('Training Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98b094e6-28a9-429e-8f79-830a081620ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter starting water level (0–100):  80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting with level: 80%\n",
      "Simulating for 10 steps:\n",
      "\n",
      "Step 1: Level=80% → Action=STOP\n",
      "Step 2: Level=75% → Action=STOP\n",
      "Step 3: Level=65% → Action=STOP\n",
      "Step 4: Level=55% → Action=FILL\n",
      "Step 5: Level=60% → Action=STOP\n",
      "Step 6: Level=50% → Action=FILL\n",
      "Step 7: Level=65% → Action=STOP\n",
      "Step 8: Level=50% → Action=FILL\n",
      "Step 9: Level=60% → Action=STOP\n",
      "Step 10: Level=55% → Action=FILL\n",
      "\\Simulation complete. Water tank control finished.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    level = int(input('Enter starting water level (0–100): '))\n",
    "    if level < 0 or level > 100:\n",
    "        raise ValueError('Water level out of range!')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    level = 50\n",
    "    print('Defaut level set to 50%.')\n",
    "\n",
    "print(f'\\nStarting with level: {level}%')\n",
    "print('Simulating for 10 steps:\\n')\n",
    "\n",
    "for step in range(10):\n",
    "    action = actions[np.argmax(Q[level // 10])]\n",
    "    print(f'Step {step+1}: Level={level}% → Action={action}')\n",
    "    level = next_level(level, action)\n",
    "\n",
    "print('\\Simulation complete. Water tank control finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79d34e-9389-40f6-8146-ad5210d2ceed",
   "metadata": {},
   "source": [
    "## Exercise: Smart Traffic Light Controller using Q-Learning \n",
    "**Objective:**\n",
    "In this exercise, you’ll design a Smart Traffic Light System that learns when to switch lights (Green/Red) based on real-time traffic conditions \n",
    "using Q-Learning. Your AI agent will balance reducing waiting time for vehicles, saving energy, and maintaining safety\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2943b-3e7f-423d-8f2c-b19cc105d1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
